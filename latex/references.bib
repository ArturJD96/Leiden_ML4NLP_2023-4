
@article{antila_vis_2014,
	title = {{THE} {VIS} {FRAMEWORK}: {ANALYZING} {COUNTERPOINT} {IN} {LARGE} {DATASETS}},
	abstract = {The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the ﬂexibility of this approach for future researchers.},
	language = {en},
	author = {Antila, Christopher and Cumming, Julie},
	year = {2014},
	file = {Antila dan Cumming - 2014 - THE VIS FRAMEWORK ANALYZING COUNTERPOINT IN LARGE.pdf:/Users/eresferro/Zotero/storage/XC5RUA3R/Antila dan Cumming - 2014 - THE VIS FRAMEWORK ANALYZING COUNTERPOINT IN LARGE.pdf:application/pdf},
}

@misc{foscarin_concept-based_2022,
	title = {Concept-{Based} {Techniques} for "{Musicologist}-friendly" {Explanations} in a {Deep} {Music} {Classifier}},
	url = {http://arxiv.org/abs/2208.12485},
	doi = {10.48550/arXiv.2208.12485},
	abstract = {Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Foscarin, Francesco and Hoedt, Katharina and Praher, Verena and Flexer, Arthur and Widmer, Gerhard},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12485 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: In Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), Bengaluru, India},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/EIB8LATV/Foscarin et al. - 2022 - Concept-Based Techniques for Musicologist-friendl.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/29WHFKCG/2208.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/CDBALWWV/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/NHZHG82L/1706.html:text/html},
}

@misc{briot_deep_2019,
	title = {Deep {Learning} {Techniques} for {Music} {Generation} -- {A} {Survey}},
	url = {http://arxiv.org/abs/1709.01620},
	doi = {10.48550/arXiv.1709.01620},
	abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Briot, Jean-Pierre and Hadjeres, Gaëtan and Pachet, François-David},
	month = aug,
	year = {2019},
	note = {arXiv:1709.01620 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Comment: 209 pages. This paper is a simplified version of the book: J.-P. Briot, G. Hadjeres and F.-D. Pachet, Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems, Springer, 2019},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/Z3F8FXVW/Briot et al. - 2019 - Deep Learning Techniques for Music Generation -- A.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/SQNBNJA7/1709.html:text/html},
}

@article{cuthbert_feature_nodate,
	title = {{FEATURE} {EXTRACTION} {AND} {MACHINE} {LEARNING}},
	abstract = {Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the “feature” capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system’s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper’s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.},
	language = {en},
	author = {Cuthbert, Michael Scott and Ariza, Christopher and Friedland, Lisa},
	file = {Cuthbert et al. - FEATURE EXTRACTION AND MACHINE LEARNING.pdf:/Users/eresferro/Zotero/storage/7BLD3W2N/Cuthbert et al. - FEATURE EXTRACTION AND MACHINE LEARNING.pdf:application/pdf},
}

@article{farbood_analysis_2001,
	title = {Analysis and {Synthesis} of {Palestrina}-{Style} {Counterpoint} {Using} {Markov} {Chains}},
	abstract = {This paper presents a novel approach to computer-generated Palestrina-style counterpoint using probabilis-tic Markov Chains. It is shown how Markov Chains ade-quately capture the rules of species counterpoint and how they can be used to synthesize species counterpoint given a cantus firmus. It is also shown how such rules can be inferred from given counterpoint examples.},
	author = {Farbood, Morwaread and Schoner, Bernd},
	month = jan,
	year = {2001},
	file = {Full Text PDF:/Users/eresferro/Zotero/storage/AKWFCHG7/Farbood and Schoner - 2001 - Analysis and Synthesis of Palestrina-Style Counter.pdf:application/pdf},
}

@inproceedings{kalonaris_computational_2020,
	address = {Online},
	title = {Computational {Linguistics} {Metrics} for the {Evaluation} of {Two}-{Part} {Counterpoint} {Generated} with {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.nlp4musa-1.9},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {Music} and {Audio} ({NLP4MusA})},
	publisher = {Association for Computational Linguistics},
	author = {Kalonaris, Stefano and McLachlan, Thomas and Aljanaki, Anna},
	editor = {Oramas, Sergio and Espinosa-Anke, Luis and Epure, Elena and Jones, Rosie and Sordo, Mohamed and Quadrana, Massimo and Watanabe, Kento},
	year = {2020},
	pages = {43--48},
	file = {Full Text PDF:/Users/eresferro/Zotero/storage/95PPLS3H/Kalonaris et al. - 2020 - Computational Linguistics Metrics for the Evaluati.pdf:application/pdf},
}

@article{mackay_toward_2002,
	title = {Toward a {Theory} of {Formal} {Function} for {Renaissance} {Music}},
	volume = {23},
	issn = {0271-8022},
	url = {https://www.jstor.org/stable/24046539},
	urldate = {2024-02-03},
	journal = {Indiana Theory Review},
	author = {MacKay, James S.},
	year = {2002},
	note = {Publisher: [Department of Music Theory, Jacobs School of Music, Indiana University, Indiana University Press, Trustees of Indiana University]},
	pages = {99--131},
	file = {JSTOR Full Text PDF:/Users/eresferro/Zotero/storage/5D9HGR8K/MacKay - 2002 - Toward a Theory of Formal Function for Renaissance.pdf:application/pdf},
}


@article{yang_evaluation_2020,
	title = {On the evaluation of generative models in music},
	volume = {32},
	issn = {0941-0643},
	url = {https://link.springer.com/epdf/10.1007/s00521-018-3849-7},
	doi = {10.1007/s00521-018-3849-7},
	abstract = {The modeling of artificial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to different tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy definition of creativity combined with varying goals of the evaluated generative systems, however, makes subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insignificant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.},
	language = {en},
	number = {9},
	urldate = {2024-02-04},
	journal = {Neural Computing and Applications},
	author = {Yang, Li-Chia and Lerch, Alexander},
	year = {2020},
	file = {Snapshot:/Users/eresferro/Zotero/storage/5AK3PJ7S/10.html:text/html},
}


@misc{hsiao_compound_2021,
	title = {Compound {Word} {Transformer}: {Learning} to {Compose} {Full}-{Song} {Music} over {Dynamic} {Directed} {Hypergraphs}},
	shorttitle = {Compound {Word} {Transformer}},
	url = {http://arxiv.org/abs/2101.02402},
	doi = {10.48550/arXiv.2101.02402},
	abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5--10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.},
	urldate = {2024-02-04},
	publisher = {arXiv},
	author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
	month = jan,
	year = {2021},
	note = {arXiv:2101.02402 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/LX76BRFX/Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Ful.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/7D4IA6UY/2101.html:text/html},
}


@inproceedings{cuthbert_feature_2011,
	title = {Feature {Extraction} and {Machine} {Learning} on {Symbolic} {Music} using the music21 {Toolkit}},
	url = {https://www.semanticscholar.org/paper/Feature-Extraction-and-Machine-Learning-on-Symbolic-Cuthbert-Ariza/cab15b3efb7cd00fc6369da36edf9d0cf48d32dc},
	abstract = {Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the “feature” capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system’s built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper’s demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.},
	urldate = {2024-02-04},
	author = {Cuthbert, M. and Ariza, C. and Friedland, Lisa},
	year = {2011},
	annote = {[TLDR] The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly, to take advantage of the system’s built-in capacities to parse diverse data formats and to manipulate complex scores.},
	file = {Full Text PDF:/Users/eresferro/Zotero/storage/C4ZHJRRC/Cuthbert et al. - 2011 - Feature Extraction and Machine Learning on Symboli.pdf:application/pdf},
}

@book{palestrina1961opere,
  title={Le opere complete},
  author={da Palestrina, G.P. and Casimiri, R. and Jeppesen, K. and Bianchi, L.},
  number={t. 30},
  lccn={42047722},
  series={Le opere complete},
  url={https://books.google.nl/books?id=t8MuAQAAIAAJ},
  year={1961},
  publisher={Fratelli Scalera}
}


@article{arthur_vicentino_2021,
	title = {Vicentino versus Palestrina: A computational investigation of voice leading across changing vocal densities},
	volume = {50},
	issn = {0929-8215, 1744-5027},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2021.1877729},
	doi = {10.1080/09298215.2021.1877729},
	shorttitle = {Vicentino versus Palestrina},
	abstract = {This paper details a corpus study examining Renaissance voice-leading practices. Palestrina’s masses are searched for progressions matching contrapuntal ‘rules’ taken from Vicentino (1555). Vicentino’s treatise provides a quasi-systematic organization of contrapuntal rules according to the minimum size of the vocal texture in which they ought to be set. Palestrina’s realizations of these progressions illustrate the exact size of vocal texture employed, enabling a direct comparison of theory and practice. The analysis reveals a general agreement, but suggests that Vicentino’s taxonomy is too strict. The results are examined in the context of uncovering guiding theoretical and perceptual principles.},
	pages = {74--101},
	number = {1},
	journaltitle = {Journal of New Music Research},
	shortjournal = {Journal of New Music Research},
	author = {Arthur, Claire},
	urldate = {2024-02-04},
	date = {2021-01-01},
	langid = {english},
	file = {Arthur - 2021 - Vicentino versus Palestrina A computational inves.pdf:/Users/arturdobija/Zotero/storage/FWGD7FRK/Arthur - 2021 - Vicentino versus Palestrina A computational inves.pdf:application/pdf},
}


@misc{zeng_musicbert_2021,
	title = {{MusicBERT}: {Symbolic} {Music} {Understanding} with {Large}-{Scale} {Pre}-{Training}},
	shorttitle = {{MusicBERT}},
	url = {http://arxiv.org/abs/2106.05630},
	doi = {10.48550/arXiv.2106.05630},
	abstract = {Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and bar-level masking strategy in MusicBERT.},
	urldate = {2024-02-04},
	publisher = {arXiv},
	author = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05630 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted by ACL 2021 Findings},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/LC2FIVY2/Zeng et al. - 2021 - MusicBERT Symbolic Music Understanding with Large.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/LX9HW4Z3/2106.html:text/html},
}


@article{neves_generating_2022,
	title = {{GENERATING} {MUSIC} {WITH} {SENTIMENT} {USING} {TRANSFORMER}-{GANS}},
	abstract = {The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ªrealisticº or ªhuman-likeº is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-{GAN} trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals.},
	author = {Neves, Pedro L T and Fornari, Jose and Florindo, João B},
	date = {2022},
	langid = {english},
	file = {Neves et al. - 2022 - GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-.pdf:/Users/arturdobija/Zotero/storage/6T2M2JRS/Neves et al. - 2022 - GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-.pdf:application/pdf},
}

@article{mennen_pattern_nodate,
	title = {Pattern recognition and machine learning based on musical information},
	author = {Mennen, Patrick},
	langid = {english},
	file = {Mennen - Pattern recognition and machine learning based on .pdf:/Users/arturdobija/Zotero/storage/3TTL88DE/Mennen - Pattern recognition and machine learning based on .pdf:application/pdf},
}

@misc{nichols_modeling_2022,
	title = {Modeling Baroque Two-Part Counterpoint with Neural Machine Translation},
	url = {http://arxiv.org/abs/2006.14221},
	abstract = {We propose a system for contrapuntal music generation based on a Neural Machine Translation ({NMT}) paradigm. We consider Baroque counterpoint and are interested in modeling the interaction between any two given parts as a mapping between a given source material and an appropriate target material. Like in translation, the former imposes some constraints on the latter, but doesn’t deﬁne it completely. We collate and edit a bespoke dataset of Baroque pieces, use it to train an attention-based neural network model, and evaluate the generated output via {BLEU} score and musicological analysis. We show that our model is able to respond with some idiomatic trademarks, such as imitation and appropriate rhythmic offset, although it falls short of having learned stylistically correct contrapuntal motion (e.g., avoidance of parallel ﬁfths) or stricter imitative rules, such as canon.},
	number = {{arXiv}:2006.14221},
	publisher = {{arXiv}},
	author = {Nichols, Eric P. and Kalonaris, Stefano and Micchi, Gianluca and Aljanaki, Anna},
	urldate = {2024-02-05},
	date = {2022-01-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.14221 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, I.2.1},
	file = {Nichols et al. - 2022 - Modeling Baroque Two-Part Counterpoint with Neural.pdf:/Users/arturdobija/Zotero/storage/YVMWIZE4/Nichols et al. - 2022 - Modeling Baroque Two-Part Counterpoint with Neural.pdf:application/pdf},
}

@article{adiloglu_machine_2007,
	title = {A machine learning approach to two-voice counterpoint composition},
	volume = {20},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705106001468},
	doi = {10.1016/j.knosys.2006.04.018},
	abstract = {Algorithmic composition of musical pieces is one of the most popular areas of computer aided music research. Various attempts have been made successfully in the area of music composition. {ArtiWcial} intelligence methods have been extensively applied in this area. Representation of musical pieces in a computer-understandable form plays an important role in computer aided music research.},
	pages = {300--309},
	number = {3},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Adiloglu, Kamil and Alpaslan, Ferda N.},
	urldate = {2024-02-05},
	date = {2007-04},
	langid = {english},
	file = {Adiloglu and Alpaslan - 2007 - A machine learning approach to two-voice counterpo.pdf:/Users/arturdobija/Zotero/storage/IKHXW8XU/Adiloglu and Alpaslan - 2007 - A machine learning approach to two-voice counterpo.pdf:application/pdf},
}

@article{huang2018music,
  title={Music Transformer: Generating Music with Long-Term Structure},
  author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Hawthorne, Curtis and Dai, Andrew M and Hoffman, Matthew D and Eck, Douglas},
  journal={arXiv preprint arXiv:1809.04281},
  year={2018}
}

@inproceedings{remi,
  author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
  title = {Pop Music Transformer: Beat-Based Modeling and Generation of Expressive Pop Piano Compositions},
  year = {2020},
  isbn = {9781450379885},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3394171.3413671},
  doi = {10.1145/3394171.3413671},
  pages = {1180–1188},
  numpages = {9},
  location = {Seattle, WA, USA},
  series = {MM '20}
}

@article{MidiLike,
  author       = {Sageev Oore and
                  Ian Simon and
                  Sander Dieleman and
                  Douglas Eck and
                  Karen Simonyan},
  title        = {This Time with Feeling: Learning Expressive Musical Performance},
  journal      = {CoRR},
  volume       = {abs/1808.03715},
  year         = {2018},
  url          = {http://arxiv.org/abs/1808.03715},
  eprinttype    = {arXiv},
  eprint       = {1808.03715},
  timestamp    = {Sun, 02 Sep 2018 15:01:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1808-03715.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{musicTuringTest,
author = {Belgum, Erik and Roads, Curtis and Chadabe, Joel and Tobenfeld, T. and Spiegel, Laurie},
year = {1989},
month = {12},
pages = {7},
title = {A Turing Test for "Musical Intelligence"?},
volume = {12},
journal = {Computer Music Journal},
doi = {10.2307/3680146}
}

@inproceedings{simssa,
author = {Fujinaga, Ichiro and Hankinson, Andrew and Cumming, Julie},
year = {2014},
month = {09},
pages = {1-3},
title = {Introduction to SIMSSA (Single Interface for Music Score Searching and Analysis)},
doi = {10.1145/2660168.2660184}
}

@article{humdrum,
 ISSN = {01489267, 15315169},
 URL = {http://www.jstor.org/stable/3681454},
 author = {David Huron},
 journal = {Computer Music Journal},
 number = {2},
 pages = {11--26},
 publisher = {The MIT Press},
 title = {Music Information Processing Using the Humdrum Toolkit: Concepts, Examples, and Lessons},
 urldate = {2024-02-05},
 volume = {26},
 year = {2002}
}

@unknown{ConceptBasedTechniques,
author = {Foscarin, Francesco and Hoedt, Katharina and Praher, Verena and Flexer, Arthur and Widmer, Gerhard},
year = {2022},
month = {08},
pages = {},
title = {Concept-Based Techniques for "Musicologist-friendly" Explanations in a Deep Music Classifier}
}

@misc{zeng2021musicbert,
      title={MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training}, 
      author={Mingliang Zeng and Xu Tan and Rui Wang and Zeqian Ju and Tao Qin and Tie-Yan Liu},
      year={2021},
      eprint={2106.05630},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}