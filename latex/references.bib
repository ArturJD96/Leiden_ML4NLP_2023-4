@article{antila_vis_2014,
	title = {{THE} {VIS} {FRAMEWORK}: {ANALYZING} {COUNTERPOINT} {IN} {LARGE} {DATASETS}},
	abstract = {The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the ﬂexibility of this approach for future researchers.},
	language = {en},
	author = {Antila, Christopher and Cumming, Julie},
	year = {2014},
	file = {Antila dan Cumming - 2014 - THE VIS FRAMEWORK ANALYZING COUNTERPOINT IN LARGE.pdf:/Users/eresferro/Zotero/storage/XC5RUA3R/Antila dan Cumming - 2014 - THE VIS FRAMEWORK ANALYZING COUNTERPOINT IN LARGE.pdf:application/pdf},
}

@misc{foscarin_concept-based_2022,
	title = {Concept-{Based} {Techniques} for "{Musicologist}-friendly" {Explanations} in a {Deep} {Music} {Classifier}},
	url = {http://arxiv.org/abs/2208.12485},
	doi = {10.48550/arXiv.2208.12485},
	abstract = {Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Foscarin, Francesco and Hoedt, Katharina and Praher, Verena and Flexer, Arthur and Widmer, Gerhard},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12485 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: In Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR 2022), Bengaluru, India},
	file = {arXiv Fulltext PDF:/Users/eresferro/Zotero/storage/EIB8LATV/Foscarin et al. - 2022 - Concept-Based Techniques for Musicologist-friendl.pdf:application/pdf;arXiv.org Snapshot:/Users/eresferro/Zotero/storage/29WHFKCG/2208.html:text/html},
}
